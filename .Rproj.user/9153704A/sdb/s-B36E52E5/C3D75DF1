{
    "contents" : "---\ntitle: \"Data Science Specialization - 8.PML - Final Project\"\nauthor: \"jmvilaverde\"\ndate: \"Monday, July 13, 2015\"\noutput: html_document\n---\n\n## Background\n\nUsing devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). \n\n##What you should submit\n\n### You should create a report describing how you built your model, \n\nFirst of all, data is extracted from web\n\n##Data Extract, Transform and Load\n\n\n* The training data for this project are available here: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\n\n* The test data are available here: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\n\n```{r extract, echo=TRUE, cache=TRUE}\n###############\n#Extract files and data#\n###############\n\ninitValues <- function(){\n        \n        #Set URL path\n        URLTraining <<- \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\"\n        URLTesting <<- \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\"\n\n        #Set csv name\n        fileTraining <<- \"training.csv\"\n        fileTesting <<- \"testing.csv\"\n}\n\nextractData <- function(){\n        \n        initValues()\n        setInternet2(use = TRUE)\n\n        #Adquire training file\n        if (!file.exists(fileTraining)) download.file(URLTraining, fileTraining)\n        trainingData <<- read.csv(fileTraining)\n        \n        #Adquire testing file\n        if (!file.exists(fileTesting)) download.file(URLTesting, fileTesting)\n        testingData <<- read.csv(fileTesting)\n\n}\n\nextractData()\n\n# names(trainingData)\n# str(trainingData)\n# summary(trainingData)\n\n\n```\n\n\n\n## Initial Data analysis\n\nBased on information extracted from:\n\n> (http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises)\n\n> Please, cite this paper to refer the WLE dataset\n\n> Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.\n\n> Read more: (http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises#ixzz3fnKRBc4a)\n\nThe objective of this data is to define quality of execution.\n\nThe data into classe variable is a factor with these 5-type:\n\nSix young health participants (`r unique(trainingData$user_name)`)  were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashion:\n\nFactor | Classification                         | Type\n------ | -------------------------------------- | -----------\nA      | exactly according to the specification | Correct\nB      | throwing the elbows to the front       | Mistake\nC      | lifting the dumbbell only halfway      | Mistake\nD      | lowering the dumbbell only halfway     | Mistake\nE      | throwing the hips to the front         | Mistake\n\nThere is a total of `r ncols(trainingData)-1` variables as predictors. \n\nAll available predictors:\n\n`r names(trainingData)`\n\nData collects information from 4 sensors (belt, arm, dumbbell, forearm), each sensor has 3 detectors (acceleration, gyroscope and magnetometer) and each detector has 3 axis (x, y, z), that makes a total of 36 variables. Other sensors predictors are derivated from these predictors.\n\nFor our analysis model are relevant:\n\n`r colnames(trainingData[,grep(\"_x$|_y$|_z$|classe\", colnames(trainingData))])`\n\nAlso, exists a user_name predictor, that indicates who is the user, including this predictor increase the prediction accuracy when the user has participated on training.\n\nuser_name\n\ncolnames(trainingData)\nOther relevant predictor is $new_window$ (indicates start of repetition) and $num_window$ (indentificates a repetition)\n\n`r colnames(trainingData[,grep(\"window$\", colnames(trainingData))])`\n\nPosition over time may be relevant, can be interesting to include timestamp predictors:\n\n`r colnames(trainingData[,grep(\"timestamp\", colnames(trainingData))])`\n\nAre proposed two model options based on these premises:\n\n* It's presumed that sensors are basic to prediction.\n* Because the prediction is about users that participates on training, include user_name as predictor have to improve accuracy.\n* When starts repetition new_window and which repetition is num_window can be relevant to prediction.\n* Timestamp predictors can be considered over final model to see the effect of it.\n\n### 1. Model sensors and user name: 36 predictors (_X, _y, _Z) + user_name.\n\nFor combined model is used a Random Forest model that is a combination of LDA, Booting and Random Forest model's predictions.\n\n\nUsing trainingData create a 60% training set, 20% testing set and 20% validating set.\n\n```{r modelSensorsUserDataSet}\n#Based on training data create training sets and test sets\nrequire(caret)\nrequire(kernlab)\nrequire(pROC) #varImp()\n\n#Create a model only using x y z axis and classe\nset.seed(1525)\n\nDataset <- trainingData[,grep(\"_x$|_y$|_z$|user|classe\", colnames(trainingData))]\n\ninBuild <- createDataPartition(y=Dataset$classe, p=0.8, list=FALSE)\nvalidation <- Dataset[-inBuild,]\n\nbuildData <- Dataset[inBuild,]\n\ninTrain <- createDataPartition(y=buildData$classe, p=0.75, list=FALSE)\ntrainingSet <- buildData[inTrain,]\ntestingSet <- buildData[-inTrain,]\n\n\ndim(trainingSet)\ndim(testingSet)\ndim(validation)\n```\n\nUse a LDA Model: Linear discriminant analysis\n\n```{r modelLDASensorsUser}\nrequire(MASS)\nmodelLDA <- train(classe ~ ., data = trainingSet, method=\"lda\")\nsave(modelLDA, file=\"models/modelLDA.rda\")\nload(\"models/modelLDA.rda\")\npredictedLDA <- predict(modelLDA, testingSet)\nconfusionMatrix(predictedLDA, testingSet$classe)\n#Accuracy : 0.6628 Kappa : 0.5721\npredictedLDAv <- predict(modelLDA, validation)\nconfusionMatrix(predictedLDAv, validation$classe)\n#Accuracy : 0.6538 Kappa : 0.5605\n```\n \nUse a GBM model: Gradient Boosting Machine\n\n```{r modelGBMSensorsUser}\nrequire(gbm)\nmodelGBM <- train(classe ~ ., data = trainingSet, method=\"gbm\")\nsave(modelGBM, file=\"models/modelGBM.rda\")\nload(\"models/modelGBM.rda\")\nvarImp(modelGBM)\npredictedGBM <- predict(modelGBM, testingSet)\nconfusionMatrix(predictedGBM, testingSet$classe)\n#Consumes a lot of computing time +- 15 minutos\n#Accuracy : 0.9123 Kappa : 0.8889\npredictedGBMv <- predict(modelGBM, validation)\nconfusionMatrix(predictedGBMv, validation$classe)\n#Accuracy : 0.9039 Kappa : 0.8782\n```\n\nUse a RF model: Random Forest.\n\n```{r modelRFSensorsUser}\nrequire(randomForest)\nmodelRF <- train(classe ~ ., data = trainingSet, method=\"rf\", do.trace=10, ntree=100)\nsave(modelRF, file=\"models/modelRF.rda\")\nload(\"models/modelRF.rda\")\npredictedRF <- predict(modelRF, testingSet)\nconfusionMatrix(predictedRF, testingSet$classe)\n#Consumes a lot of computing time 6 minutes\n#Accuracy : 0.9801 Kappa : 0.9748 Mcnemar's Test P-Value : NA\npredictedRFv <- predict(modelRF, validation)\nconfusionMatrix(predictedRFv, validation$classe)\n# Accuracy : 0.9809 Kappa : 0.9758 Mcnemar's Test P-Value : NA\n``` \n\nCombined model: Random forest from other model's predictions\n\n```{r modelCombined}\n#Model combined\npredCompDF <- data.frame(predictedLDA, predictedGBM, predictedRF, y=testingSet$classe)\ncombMod <- train(y ~ ., data = predCompDF, method=\"rf\", do.trace=10, ntree=100)\nsave(\"combMod\", file=\"models/combMod.rda\")\nload(\"models/combMod.rda\")\n#Time to predict:\ncombPred <- predict(combMod, testingSet)\nconfusionMatrix(combPred, testingSet$classe)\n#Accuracy : 0.9801 Kappa : 0.9748\ncombPredv <- predict(combMod, validation)\nconfusionMatrix(combPredv, validation$classe)\n#Accuracy : 0.9801 Kappa : 0.9748\n\n#Prediction of testingData Set to upload as result\ntpredictedLDA <- predict(modelLDA, testingData)\ntpredictedGBM <- predict(modelGBM, testingData)\ntpredictedRF <- predict(modelRF, testingData)\n\nnewdataset <- data.frame(tpredictedLDA, tpredictedGBM, tpredictedRF)\ncolnames(newdataset) <- colnames(predCompDF[,1:3])\n\ntpredictedComb <- predict(combMod, newdata = newdataset)\n\nresult <- rbind(seq(1:20), as.character(tpredictedLDA), as.character(tpredictedGBM), as.character(tpredictedRF), as.character(tpredictedComb))\n\n```\n\nEstimated accuracy and Kappa with this predictors (taking lower value between testing and validating):\n\nModel | Accuracy | Kappa  | Time to process\n----- | -------- | ------ | ---------\nLDA   | 0.6538   | 0.5605 | \nGBM   | 0.9039   | 0.8782 | 15 minutes\nRF    | 0.9801   | 0.9748 | 15 minutes\nComb  | 0.9801   | 0.9748 | \n\nPrediction for testing set: `r tpredictedComb #B A B A A E D B A A B C B A E E A B B B`\n\n### 2. Model sensors + user_name + window\n\nFor combined model is used a Random Forest model that is a combination of LDA, Booting and Random Forest model's predictions.\n\nUsing trainingData create a 60% training set, 20% testing set and 20% validating set.\n\n```{r modelSensorsUserWindowDataSet}\n\n#Based on training data create training sets and test sets\nrequire(caret)\nrequire(kernlab)\nrequire(pROC) #varImp()\n\n#Create a model only using x y z axis and user_name, window, classe\nset.seed(1525)\n\nDataset2 <- trainingData[,grep(\"_x$|_y$|_z$|classe|user|window\", colnames(trainingData))]\n\ninBuild2 <- createDataPartition(y=Dataset2$classe, p=0.8, list=FALSE)\nvalidation2 <- Dataset2[-inBuild2,]\n\nbuildData2 <- Dataset2[inBuild2,]\n\ninTrain2 <- createDataPartition(y=buildData2$classe, p=0.75, list=FALSE)\ntrainingSet2 <- buildData2[inTrain2,]\ntestingSet2 <- buildData2[-inTrain2,]\n\n\ndim(trainingSet2)\ndim(testingSet2)\ndim(validation2)\n```\n\n```{r modelLDASensorsUserWindow}\n#Model LDA\nrequire(MASS)\nmodelLDA2 <- train(classe ~ ., data = trainingSet2, method=\"lda\")\nsave(modelLDA2, file=\"models/modelLDA2.rda\")\nload(\"models/modelLDA2.rda\")\n\npredictedLDA2 <- predict(modelLDA2, testingSet2)\nconfusionMatrix(predictedLDA2, testingSet2$classe)\n#Accuracy : 0.6702 Kappa : 0.5815\npredictedLDA2v <- predict(modelLDA2, validation2)\nconfusionMatrix(predictedLDA2v, validation2$classe)\n#Accuracy : 0.6643 Kappa : 0.5735  \n```\n\n```{r modelGBMSensorsUserWindow}\n#Third model GBM\nrequire(gbm)\nSys.time()\nmodelGBM2 <- train(classe ~ ., data = trainingSet2, method=\"gbm\")\nSys.time()\nsave(modelGBM2, file=\"models/modelGBM2.rda\")\nload(\"models/modelGBM2.rda\")\n\npredictedGBM2 <- predict(modelGBM2, testingSet2)\nconfusionMatrix(predictedGBM2, testingSet2$classe)\n#Consumes a lot of computing time 13:51 - 14:12\n#Accuracy : 0.9862 Kappa : 0.9826 Mcnemar's Test P-Value : NA\npredictedGBM2v <- predict(modelGBM2, validation2)\nconfusionMatrix(predictedGBM2v, validation2$classe)\n# Accuracy : 0.9845 Kappa : 0.9803 Mcnemar's Test P-Value : NA\n```\n\n```{r modelRFSensorsUserWindow}\n#Model Random Forest\nrequire(randomForest)\nSys.time()\nmodelRF2 <- train(classe ~ ., data = trainingSet2, method=\"rf\", do.trace=10, ntree=100)\nSys.time()\nsave(modelRF2, file=\"models/modelRF2.rda\")\nload(\"models/modelRF2.rda\")\n\npredictedRF2 <- predict(modelRF2, testingSet2)\nconfusionMatrix(predictedRF2, testingSet2$classe)\n#Consumes a lot of computing time 14:18 - 14:27\n#Accuracy : 0.9954 Kappa : 0.9942\npredictedRF2v <- predict(modelRF2, validation2)\nconfusionMatrix(predictedRF2v, validation2$classe)\n#Accuracy : 0.9954 Kappa : 0.9942\n```\n\n```{r modelCombSensorsUserWindow}\n#Model combined\npredCompDF2 <- data.frame(predictedLDA2, predictedGBM2, predictedRF2, classe=testingSet2$classe)\ncombMod2 <- train(classe ~ ., data = predCompDF2, method=\"rf\", do.trace=10, ntree=100)\nsave(combMod2, file=\"models/combMod2.rda\")\nload(\"models/combMod2.rda\")\n#Time to predict:\ncombPred2 <- predict(combMod2, testingSet2)\nconfusionMatrix(combPred2, testingSet2$classe)\n# Accuracy : 0.9959 Kappa : 0.9948\nvalidationDataComb <- data.frame(predictedLDA2v, predictedGBM2v, predictedRF2v, y=validation2$classe)\ncolnames(validationDataComb) <- colnames(predCompDF2)\ncombPred2v <- predict(combMod2, validationDataComb)\nconfusionMatrix(combPred2v, validationDataComb$classe)\n#Accuracy : 0.9962 Kappa : 0.9952\n\n#Prediction of testingData Set to upload as result\ntpredictedLDA2 <- predict(modelLDA2, testingData)\ntpredictedGBM2 <- predict(modelGBM2, testingData)\ntpredictedRF2 <- predict(modelRF2, testingData)\n\nnewdataset2 <- data.frame(tpredictedLDA2, tpredictedGBM2, tpredictedRF2)\ncolnames(newdataset2) <- colnames(predCompDF2[,1:3])\n\ntpredictedComb2 <- predict(combMod2, newdata = newdataset2)\n\nresult2 <- cbind(seq(1:20), as.character(tpredictedLDA2), as.character(tpredictedGBM2), as.character(tpredictedRF2), as.character(tpredictedComb2))\n\nresult1\nresult2\ntpredictedComb2\n```\n\n\n\n\n\n\n```{r submit}\npml_write_files = function(x){\n  n = length(x)\n  for(i in 1:n){\n    filename = paste0(\"prediction/problem_id_\",i,\".txt\")\n    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)\n  }\n}\n\npml_write_files(tpredictedComb2)\n\n\n```\n\n\n\n### how you used cross validation, \n\n### what you think the expected out of sample error is, \n\n### and why you made the choices you did. \n\n\n### You will also use your prediction model to predict 20 different test cases. \n\n> 1. Your submission should consist of a link to a Github repo with your R markdown and compiled HTML file describing your analysis. Please constrain the text of the writeup to < 2000 words and the number of figures to be less than 5. It will make it easier for the graders if you submit a repo with a gh-pages branch so the HTML page can be viewed online (and you always want to make it easy on graders :-).\n\n\n\n",
    "created" : 1436988524237.000,
    "dirty" : true,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1484482947",
    "id" : "C3D75DF1",
    "lastKnownWriteTime" : 1436987929,
    "path" : "D:/Privado/repos/8.PracticalMachineLearning/DataScienceSpecialization-8.PML-FinalProject/FinalProject.Rmd",
    "project_path" : "FinalProject.Rmd",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}