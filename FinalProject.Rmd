---
title: "Classification of execution of exercise Unilateral Dumbbell Biceps Curl"
subtitle: "Data Science Specialization - Practical Machine Learning - Final Project"
author: "by jmvilaverde"
date: "Thursday, July 16, 2015"
output: html_document
---

## Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, our goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. 

More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 

***

## Model creation

### Extract, transform and load data

* The training data for this project are available here: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

* The test data are available here: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

_Code 01. Extract files and data._
```{r extract, echo=TRUE, cache=TRUE}
initValues <- function(){
        
        #Set URL path
        URLTraining <<- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
        URLTesting <<- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

        #Set csv name
        fileTraining <<- "training.csv"
        fileTesting <<- "testing.csv"
}

extractData <- function(){
        
        #Init values and setup access to https
        initValues()
        setInternet2(use = TRUE)

        #Adquire training file
        if (!file.exists(fileTraining)) download.file(URLTraining, fileTraining)
        trainingData <<- read.csv(fileTraining)
        
        #Adquire testing file
        if (!file.exists(fileTesting)) download.file(URLTesting, fileTesting)
        testingData <<- read.csv(fileTesting)

}

extractData()
```

***

### Initial Data analysis

Based on information and data extracted from: (http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises)

> ###### Please, cite this paper to refer the WLE dataset: Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.
> ###### Read more: (http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises#ixzz3fnKRBc4a)

The objective of this data is to define quality of execution.

Six young health participants **(`r unique(trainingData$user_name)`)** were asked to perform **one set of 10 repetitions** of the **Unilateral Dumbbell Biceps Curl** in **five different fashion** that was stored into **classe** variable with these 5 types:

Fashion | Classification                         | Type
------- | -------------------------------------- | -----------
A       | exactly according to the specification | Correct
B       | throwing the elbows to the front       | Mistake
C       | lifting the dumbbell only halfway      | Mistake
D       | lowering the dumbbell only halfway     | Mistake
E       | throwing the hips to the front         | Mistake

There is a total of **`r ncol(trainingData)-1`** variables as predictors. 

Data collects information from **4 sensors (belt, arm, dumbbell, forearm)**, each sensor has **3 detectors (acceleration, gyroscope and magnetometer)** and each detector has **3 axis (x, y, z)**, that makes a total of **36 variables**. Other sensors variables are derivated from these variables.

For our analysis model are selected as relevant sensors predictors these variables:

> ##### `r colnames(trainingData[,grep("_x$|_y$|_z$", colnames(trainingData))])`

Also, exists a **user_name** predictor, that indicates who is the user, including this predictor increases the prediction accuracy when the user has participated on training.

Other relevant predictors are **new\_window** (indicates start of repetition) and **num\_window** (indentificates a repetition).

***

### Model: sensors and user name: 39 predictors: X, y and z axis for sensors + user_name + window.

#### Model Proposal

Model is proposed based on these premises:

* It's presumed that sensors are basic to prediction.
* Because the prediction is about users that participates on training, include user_name as predictor have to improve accuracy.
* When starts repetition new\_window and which repetition is num\_window is relevant to accuracy of prediction. A model tested without these predictors have lower accuracy than model that includes it (98.01% vs 99.59% over testing set).

Model is a combined model Random Forest that is a combination of LDA, GBM and Random Forest model's predictions.

#### Cross validation 

* Dataset for training, testing and validanting contains 39 preditors and classe.
* Separate Training Data into three sets: 60% training set, 20% testing set and 20% validating set.
* Validation set is used to confirm the results obtained with testing set.

_Code 02. Creation of training, testing and validating sets._
```{r modelSensorsUserWindowDataSet, message=FALSE}

require(caret)
require(kernlab)
require(pROC)

#Seed to be used to obtain the same results
set.seed(1525)

Dataset2 <- trainingData[,grep("_x$|_y$|_z$|classe|user|window", colnames(trainingData))]

inBuild2 <- createDataPartition(y=Dataset2$classe, p=0.8, list=FALSE)
validation2 <- Dataset2[-inBuild2,]

buildData2 <- Dataset2[inBuild2,]

inTrain2 <- createDataPartition(y=buildData2$classe, p=0.75, list=FALSE)
trainingSet2 <- buildData2[inTrain2,]
testingSet2 <- buildData2[-inTrain2,]

```

Set             | Training                 | Testing                 | Validating
--------------- | ------------------------ | ----------------------- | -----------------------
Dimensions      | `r dim(trainingSet2)[1]` | `r dim(testingSet2)[1]` | `r dim(validation2)[1]`
%               | 60%                      | 20%                     | 20%


#### LDA Model: Linear discriminant analysis

_Code 03. LDA Model._
```{r modelLDASensorsUserWindow, message=FALSE}
require(MASS)

#Section to load a stored model, to avoid to redo the model
load("models/modelLDA2.rda")

#If doesn't exists, create and train the model
if (!exists("modelLDA2")) {
        modelLDA2 <- train(classe ~ ., data = trainingSet2, method="lda")
        save(modelLDA2, file="models/modelLDA2.rda")
}

#Predict over testing set
predictedLDA2 <- predict(modelLDA2, testingSet2)
cmLDA2t <- confusionMatrix(predictedLDA2, testingSet2$classe)

#Predict over validating set
predictedLDA2v <- predict(modelLDA2, validation2)
cmLDA2v <- confusionMatrix(predictedLDA2v, validation2$classe)
```



#### Model GBM: Gradient Boosting Machine

_Code 04. GBM Model._
```{r modelGBMSensorsUserWindow, message=FALSE}
require(gbm)

#Section to load a stored model, to avoid to redo the model
load("models/modelGBM2.rda")

#If doesn't exists, create and train the model
if (!exists("modelGBM2")) {
        modelGBM2 <- train(classe ~ ., data = trainingSet2, method="gbm")
        save(modelGBM2, file="models/modelGBM2.rda")
}

#Predict over testing set
predictedGBM2 <- predict(modelGBM2, testingSet2)
cmGBM2t <- confusionMatrix(predictedGBM2, testingSet2$classe)

#Predict over validating set
predictedGBM2v <- predict(modelGBM2, validation2)
cmGBM2v <- confusionMatrix(predictedGBM2v, validation2$classe)
```

#### RF model: Random Forest.

* Parameters: do.trace=10, ntree = 100

_Code 05. RF Model._
```{r modelRFSensorsUserWindow, message=FALSE}
require(randomForest)

#Section to load a stored model, to avoid to redo the model
load("models/modelRF2.rda")

#If doesn't exists, create and train the model
if(!exists("modelRF2")) {
        modelRF2 <- train(classe ~ ., data = trainingSet2, method="rf", do.trace=10, ntree=100)
        save(modelRF2, file="models/modelRF2.rda")
}

#Predict over testing set
predictedRF2 <- predict(modelRF2, testingSet2)
cmRF2t <- confusionMatrix(predictedRF2, testingSet2$classe)

#Predict over validating set
predictedRF2v <- predict(modelRF2, validation2)
cmRF2v <- confusionMatrix(predictedRF2v, validation2$classe)
```

#### Combined model: Random forest from other model's predictions

* Parameters: do.trace=10, ntree = 100

_Code 06. Combined Model Random forest._
```{r modelCombSensorsUserWindow, message=FALSE}
predCompDF2 <- data.frame(predictedLDA2, predictedGBM2, predictedRF2, classe=testingSet2$classe)

#Section to load a stored model, to avoid to redo the model
load("models/combMod2.rda")

#If doesn't exists, create and train the model
if(!exists("combMod2")) {
        combMod2 <- train(classe ~ ., data = predCompDF2, method="rf", do.trace=10, ntree=100)
        save(combMod2, file="models/combMod2.rda")
}

#Predict over testing set
combPred2 <- predict(combMod2, testingSet2)
cmComb2t <- confusionMatrix(combPred2, testingSet2$classe)

#Predict over validating set
validationDataComb <- data.frame(predictedLDA2v, predictedGBM2v, predictedRF2v, y=validation2$classe)
colnames(validationDataComb) <- colnames(predCompDF2)
combPred2v <- predict(combMod2, validationDataComb)
cmComb2v <- confusionMatrix(combPred2v, validationDataComb$classe)
```


_Figure 01. Final combined model_
```{r finalModel, echo=FALSE} 
combMod2$finalModel
```



_Figure 02. Estimated accuracy for testing set with this predictors._

Model | Accuracy                                         | Accuracy 95%
----- | ------------------------------------------------ | ------------------------- 
LDA   | `r round((cmLDA2t$overall["Accuracy"])*100,2)`%  | `r round((cmLDA2t$overall[c("AccuracyLower", "AccuracyUpper")])*100,2)`%
GBM   | `r round((cmGBM2t$overall["Accuracy"])*100,2)`%  | `r round((cmGBM2t$overall[c("AccuracyLower", "AccuracyUpper")])*100,2)`%
RF    | `r round((cmRF2t$overall["Accuracy"])*100,2)`%   | `r round((cmRF2t$overall[c("AccuracyLower", "AccuracyUpper")])*100,2)`%
Combined | `r round((cmComb2t$overall["Accuracy"])*100,2)`% | `r round((cmComb2t$overall[c("AccuracyLower", "AccuracyUpper")])*100,2)`%



_Figure 03. Final combined model - Out of sample error (OSE) (1-Accuracy) over testing & validating set._

Set        | Out of sample error                                | Out of sample error 95% confidence interval
---------- | -------------------------------------------------- | --------------------------------------------
Testing    | `r round((1-cmComb2t$overall["Accuracy"])*100,2)`% | `r round((1-cmComb2t$overall[c("AccuracyLower", "AccuracyUpper")])*100,2)`% 
Validating | `r round((1-cmComb2v$overall["Accuracy"])*100,2)`% | `r round((1-cmComb2v$overall[c("AccuracyLower", "AccuracyUpper")])*100,2)`%

***

## Prediction to submit

_Code 07. Prediction of classe._
```{r predictions, message=FALSE}
#Prediction of testingData Set to upload as result
tpredictedLDA2 <- predict(modelLDA2, testingData)
tpredictedGBM2 <- predict(modelGBM2, testingData)
tpredictedRF2 <- predict(modelRF2, testingData)

newdataset2 <- data.frame(tpredictedLDA2, tpredictedGBM2, tpredictedRF2)
colnames(newdataset2) <- colnames(predCompDF2[,1:3])

tpredictedComb2 <- predict(combMod2, newdata = newdataset2)
```

> Prediction results for combined model: `r tpredictedComb2`